{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set appropriate sys path and import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List existing pipeline experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiments': [{'created_at': datetime.datetime(2019, 9, 23, 9, 24, 45, tzinfo=tzlocal()),\n",
       "                  'description': 'All runs created without specifying an '\n",
       "                                 'experiment will be grouped here.',\n",
       "                  'id': 'db6840d7-a728-447a-9892-8f537ad8c4ed',\n",
       "                  'name': 'Default'},\n",
       "                 {'created_at': datetime.datetime(2019, 10, 9, 7, 13, 37, tzinfo=tzlocal()),\n",
       "                  'description': None,\n",
       "                  'id': '9f6357a6-1e2f-461a-a757-b180ecdf2b1c',\n",
       "                  'name': 'Dkube - Dental pl'},\n",
       "                 {'created_at': datetime.datetime(2019, 10, 17, 7, 22, 25, tzinfo=tzlocal()),\n",
       "                  'description': None,\n",
       "                  'id': 'f15e4970-795f-41b9-85f8-40df2c9b79c2',\n",
       "                  'name': 'Chicao Taxi Cab pl'},\n",
       "                 {'created_at': datetime.datetime(2019, 10, 17, 14, 22, 59, tzinfo=tzlocal()),\n",
       "                  'description': None,\n",
       "                  'id': '3e87dc14-6663-42fb-bb5d-9cd7198a1732',\n",
       "                  'name': 'book-classification'},\n",
       "                 {'created_at': datetime.datetime(2019, 10, 17, 14, 31, 26, tzinfo=tzlocal()),\n",
       "                  'description': None,\n",
       "                  'id': '9fb3ad71-c8ab-4a2c-a78b-d684f93f2c5e',\n",
       "                  'name': 'book classification pl'}],\n",
       " 'next_page_token': None,\n",
       " 'total_size': 5}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/9fb3ad71-c8ab-4a2c-a78b-d684f93f2c5e\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment = client.create_experiment(name='book classification pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pipeline DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Kubeflow Pipeline Example\"\"\"\n",
    "\n",
    "import os\n",
    "from kfp import onprem\n",
    "\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "\n",
    "# pylint: disable=line-too-long\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "\n",
    "from tfx.components.transform.component import Transform\n",
    "\n",
    "from tfx.proto import trainer_pb2  # Step 5\n",
    "from tfx.components.trainer.component import Trainer  # Step 5\n",
    "\n",
    "from tfx.proto import evaluator_pb2  # Step 6\n",
    "from tfx.components.evaluator.component import Evaluator  # Step 6\n",
    "\n",
    "from tfx.proto import pusher_pb2  # Step 7\n",
    "from tfx.components.model_validator.component import ModelValidator  # Step 7\n",
    "from tfx.components.pusher.component import Pusher  # Step 7\n",
    "from tfx.utils.dsl_utils import csv_input\n",
    "\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.kubeflow.proto import kubeflow_pb2\n",
    "from tfx.orchestration.kubeflow import kubeflow_dag_runner\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from ml_metadata.metadata_store import metadata_store\n",
    "from typing import Text\n",
    "\n",
    "# pylint: enable=line-too-long\n",
    "\n",
    "# Directory and data locations (uses Google Cloud Storage).\n",
    "_input_bucket = '/book-classification/data/'\n",
    "_utils_bucket = '/book-classification/tfx/'\n",
    "_output_bucket = '/book-classification'\n",
    "_pipeline_root = os.path.join(_output_bucket, 'tfx')\n",
    "pipeline_name = 'complaint_model_pipeline_kubeflow'\n",
    "\n",
    "# Python module file to inject customized logic into the TFX components. The\n",
    "# Transform and Trainer both require user-defined functions to run successfully.\n",
    "# Copy this from the current directory to a GCS bucket and update the location\n",
    "# below.\n",
    "pipeline_module_file = os.path.join(_utils_bucket, 'utils.py')\n",
    "\n",
    "# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "# _serving_model_dir = os.path.join(\n",
    "#     _output_bucket, 'serving_model/taxi_bigquery')\n",
    "\n",
    "# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "_serving_model_dir = os.path.join(\n",
    "    _output_bucket, 'serving_model/complaint_model')\n",
    "\n",
    "\n",
    "def _get_kubeflow_metadata_config(pipeline_name: Text\n",
    "                                  ) -> kubeflow_pb2.KubeflowMetadataConfig:\n",
    "    config = kubeflow_pb2.KubeflowMetadataConfig()\n",
    "    config.mysql_db_service_host.value = '10.233.64.94'\n",
    "    config.mysql_db_service_port.value = '3306'\n",
    "    config.mysql_db_name.value = _get_mlmd_db_name(pipeline_name)\n",
    "    config.mysql_db_user.value = 'root'\n",
    "    config.mysql_db_password.value = ''\n",
    "    return config\n",
    "\n",
    "def _get_metadata_store(pipeline_name: Text\n",
    "                        ) -> metadata_store_pb2.ConnectionConfig:\n",
    "    config = metadata_store_pb2.ConnectionConfig()\n",
    "    config.mysql.host = '10.233.64.94'\n",
    "    config.mysql.port = 3306\n",
    "    config.mysql.database = _get_mlmd_db_name(pipeline_name)\n",
    "    config.mysql.user = 'root'\n",
    "    config.mysql.password = ''\n",
    "    store = metadata_store.MetadataStore(config)\n",
    "    return store\n",
    "\n",
    "\n",
    "def _get_mlmd_db_name(pipeline_name: Text):\n",
    "    # MySQL DB names must not contain '-' while k8s names must not contain '_'.\n",
    "    # So we replace the dashes here for the DB name.\n",
    "    valid_mysql_name = pipeline_name.replace('-', '_')\n",
    "    # MySQL database name cannot exceed 64 characters.\n",
    "    return 'mlmd_{}'.format(valid_mysql_name[-59:])\n",
    "\n",
    "\n",
    "def _create_pipeline():\n",
    "    \"\"\"Implements the complaint model pipeline with TFX and Kubeflow Pipelines.\"\"\"\n",
    "\n",
    "    # Brings data into the pipeline or otherwise joins/converts training data.\n",
    "    # example_gen = BigQueryExampleGen(query=_query)\n",
    "\n",
    "    examples = csv_input(_input_bucket)\n",
    "\n",
    "    # Brings data into the pipeline or otherwise joins/converts training data.\n",
    "    example_gen = CsvExampleGen(input_base=examples)\n",
    "\n",
    "    # Computes statistics over data for visualization and example validation.\n",
    "    statistics_gen = StatisticsGen(input_data=example_gen.outputs.examples)\n",
    "\n",
    "    # Generates schema based on statistics files.\n",
    "    infer_schema = SchemaGen(stats=statistics_gen.outputs.output)\n",
    "\n",
    "    # Performs anomaly detection based on statistics and data schema.\n",
    "    validate_stats = ExampleValidator(\n",
    "        stats=statistics_gen.outputs.output,\n",
    "        schema=infer_schema.outputs.output)\n",
    "\n",
    "    # Performs transformations and feature engineering in training and serving.\n",
    "    transform = Transform(\n",
    "        input_data=example_gen.outputs.examples,\n",
    "        schema=infer_schema.outputs.output,\n",
    "        module_file=pipeline_module_file)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        module_file=pipeline_module_file,\n",
    "        transformed_examples=transform.outputs.transformed_examples,\n",
    "        schema=infer_schema.outputs.output,\n",
    "        transform_output=transform.outputs.transform_output,\n",
    "        train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "        eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
    "\n",
    "    # Uses TFMA to compute a evaluation statistics over features of a model.\n",
    "    model_analyzer = Evaluator(\n",
    "        examples=example_gen.outputs.examples,\n",
    "        model_exports=trainer.outputs.output,\n",
    "        feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[\n",
    "            evaluator_pb2.SingleSlicingSpec(\n",
    "                column_for_slicing=['trip_start_hour'])\n",
    "        ]))\n",
    "\n",
    "    # Performs quality validation of a candidate model (compared to a baseline).\n",
    "    model_validator = ModelValidator(\n",
    "        examples=example_gen.outputs.examples,\n",
    "        model=trainer.outputs.output)\n",
    "\n",
    "    pusher = Pusher(\n",
    "        model_export=trainer.outputs.output,\n",
    "        model_blessing=model_validator.outputs.blessing,\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=_serving_model_dir)))\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=_pipeline_root,\n",
    "        metadata_connection_config=_get_metadata_store(pipeline_name),\n",
    "        components=[\n",
    "            example_gen, statistics_gen, infer_schema, validate_stats,\n",
    "            transform, trainer, model_analyzer, model_validator, pusher\n",
    "        ],\n",
    "        additional_pipeline_args={\n",
    "            'beam_pipeline_args': [\n",
    "                '--runner=DirectRunner',\n",
    "                '--experiments=shuffle_mode=auto',\n",
    "                '--temp_location=' + os.path.join(_output_bucket, 'tmp')\n",
    "            ],\n",
    "        },\n",
    "        log_root='/var/tmp/tfx/logs'\n",
    "    )\n",
    "\n",
    "\n",
    "mount_volume_op = onprem.mount_pvc('book-classification-claim',\n",
    "                                   'book-classification',\n",
    "                                   '/book-classification')\n",
    "config = kubeflow_dag_runner.KubeflowDagRunnerConfig(\n",
    "    pipeline_operator_funcs=[mount_volume_op],\n",
    "    kubeflow_metadata_config=_get_kubeflow_metadata_config(pipeline_name)\n",
    ")\n",
    "\n",
    "kubeflow_dag_runner.KubeflowDagRunner(config=config).run(_create_pipeline())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/e01ca9fc-f17a-11e9-bdc2-42010a8e0009\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(experiment.id, 'book-classification-1', 'complaint_model_pipeline_kubeflow.tar.gz', params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
