{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please wait till this cell completes and then run next cells. This just need to be run once per active kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RELEASE_VERSION=0.1.18\n",
      "Collecting https://storage.googleapis.com/ml-pipeline/release/0.1.18/kfp.tar.gz\n",
      "  Using cached https://storage.googleapis.com/ml-pipeline/release/0.1.18/kfp.tar.gz (152 kB)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.28.1)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes<=9.0.0,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (9.0.0)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (2.9.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.14.1)\n",
      "Requirement already satisfied, skipping upgrade: requests_toolbelt>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.18) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.18) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (46.1.3.post20200325)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.1.18) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.18) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.18) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.18) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.1.18) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth>=1.6.1->kfp==0.1.18) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (1.51.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (3.11.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (2020.1)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-0.1.18-py3-none-any.whl size=242154 sha256=1c34b5e997e4d2852e27bb9ab830d7501866ce28436d86a75f4e066cd9001f58\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-l1p4h0c4/wheels/89/83/34/24a09b34340dfb7efecae25ca19424c615d5f46654cde53432\n",
      "Successfully built kfp\n",
      "Installing collected packages: kfp\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 0.1.18\n",
      "    Uninstalling kfp-0.1.18:\n",
      "      Successfully uninstalled kfp-0.1.18\n",
      "Successfully installed kfp-0.1.18\n"
     ]
    }
   ],
   "source": [
    "%env RELEASE_VERSION=0.1.18\n",
    "!pip install https://storage.googleapis.com/ml-pipeline/release/${RELEASE_VERSION}/kfp.tar.gz --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List existing pipeline experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiments': [{'created_at': datetime.datetime(2020, 6, 17, 4, 53, 17, tzinfo=tzlocal()),\n",
       "                  'description': 'All runs created without specifying an '\n",
       "                                 'experiment will be grouped here.',\n",
       "                  'id': '05814128-3167-4c1f-aeff-3c3c6db8c6dc',\n",
       "                  'name': 'Default'}],\n",
       " 'next_page_token': None,\n",
       " 'total_size': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dkube REGRESSION experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/16317a94-d321-4b2e-8eab-bbee6fdb59fd\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regression_experiment = client.create_experiment(name='Dkube - steel pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e regression Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "from kfp.components._yaml_utils import load_yaml\n",
    "from kfp.components._yaml_utils import dump_yaml\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "def _component(stage, name):\n",
    "    with open('../components/{}/component.yaml'.format(stage), 'rb') as stream:\n",
    "        cdict = load_yaml(stream)\n",
    "        cdict['name'] = name\n",
    "        cyaml = dump_yaml(cdict)\n",
    "        return components.load_component_from_text(cyaml)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='dkube-steel-pl',\n",
    "    description='steel defect detection pipeline with dkube components'\n",
    ")\n",
    "\n",
    "def d3pipeline(\n",
    "    access_url,\n",
    "    user,\n",
    "    auth_token,\n",
    "    git_token,\n",
    "    #severstal preprocess\n",
    "    severstal_preprocess_script=\"python steel/preprocessing/preprocessing.py\",\n",
    "    severstal_preprocess_datasets=json.dumps([\"steel-data\"]),\n",
    "    severstal_preprocess_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    severstal_preprocess_outputs=json.dumps([\"steel-preprocessed\"]),\n",
    "    severstal_preprocess_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    \n",
    "    #severstal split\n",
    "    severstal_split_script=\"python steel/split/split.py\",\n",
    "    severstal_split_datasets=json.dumps([\"steel-preprocessed\"]),\n",
    "    severstal_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    severstal_split_outputs=json.dumps([\"steel-train\", \"steel-test\"]),\n",
    "    severstal_split_output_mounts=json.dumps([\"/opt/dkube/output/train\", \"/opt/dkube/output/test\"]),    \n",
    "    \n",
    "    #Training\n",
    "    #In notebook DKUBE_USER_ACCESS_TOKEN is automatically picked up from env variable\n",
    "    #By default tf v1.14 image is used here, v1.13 or v1.14 can be used. \n",
    "    #Or any other custom image name can be supplied.\n",
    "    #For custom private images, please input username/password\n",
    "    training_container=json.dumps({'image':'docker.io/ocdr/d3-datascience-tf-cpu:v1.14', 'username':'', 'password': ''}),\n",
    "    #Name of the workspace in dkube. Update accordingly if different name is used while creating a workspace in dkube.\n",
    "    training_program=\"steel\",\n",
    "    #Script to run inside the training container    \n",
    "    training_script=\"python steel/model/model.py\",\n",
    "    #Input datasets for training. Update accordingly if different name is used while creating dataset in dkube.    \n",
    "    training_datasets=json.dumps([\"steel-train\"]),\n",
    "    training_input_dataset_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    training_outputs=json.dumps([\"resUnet\"]),\n",
    "    training_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    #Request gpus as needed. Val 0 means no gpu, then training_container=docker.io/ocdr/dkube-datascience-tf-cpu:v1.12    \n",
    "    training_gpus=0,\n",
    "    #Any envs to be passed to the training program    \n",
    "    training_envs=json.dumps([{\"steps\": 100}]),\n",
    "    \n",
    "    #Evaluation\n",
    "    evaluation_script=\"python steel/evaluation/evaluation.py\",\n",
    "    evaluation_datasets=json.dumps([\"steel-test\"]),\n",
    "    evaluation_input_dataset_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    evaluation_models=json.dumps([\"resUnet\"]),\n",
    "    evaluation_input_model_mounts=json.dumps([\"/opt/dkube/model\"]),\n",
    "    \n",
    "    #Serving\n",
    "    #Device to be used for serving - dkube mnist example trained on gpu needs gpu for serving else set this param to 'cpu'\n",
    "    serving_device='cpu',\n",
    "    serving_container=json.dumps({'image':'docker.io/ocdr/steel-example-preprocess:030720', 'username':'', 'password': ''})):\n",
    "    \n",
    "    create_resource = _component('setup', 'steel-setup')(access_url,\n",
    "                                               auth_token, git_token,\n",
    "                                               user)\n",
    "    \n",
    "    severstal_preprocess = _component('preprocess', 'steel-preprocess')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=severstal_preprocess_script,\n",
    "                                      datasets=severstal_preprocess_datasets, outputs=severstal_preprocess_outputs,\n",
    "                                      input_dataset_mounts=severstal_preprocess_input_mounts, output_mounts=severstal_preprocess_output_mounts).after(create_resource)\n",
    "                                      \n",
    "    severstal_split  = _component('preprocess', 'steel-split')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=severstal_split_script,\n",
    "                                      datasets=severstal_split_datasets, outputs=severstal_split_outputs,\n",
    "                                      input_dataset_mounts=severstal_split_input_mounts,\n",
    "                                      output_mounts=severstal_split_output_mounts).after(severstal_preprocess)\n",
    "                                      \n",
    "    train       = _component('training', 'steel-model-training')(auth_token, training_container,\n",
    "                                    program=training_program, run_script=training_script,\n",
    "                                    datasets=training_datasets, outputs=training_outputs,\n",
    "                                    input_dataset_mounts=training_input_dataset_mounts,\n",
    "                                    output_mounts=training_output_mounts,\n",
    "                                    ngpus=training_gpus,\n",
    "                                    envs=training_envs).after(severstal_split)\n",
    "    evaluate    = _component('training', 'steel-model-eval')(auth_token, training_container,\n",
    "                                    program=training_program, run_script=evaluation_script,\n",
    "                                    datasets=evaluation_datasets,\n",
    "                                    input_dataset_mounts=evaluation_input_dataset_mounts,\n",
    "                                    models=evaluation_models,\n",
    "                                    input_model_mounts=evaluation_input_model_mounts,\n",
    "                                    ngpus=training_gpus,\n",
    "                                    envs=training_envs).after(train)\n",
    "    serving     = _component('serving', 'model-serving')(auth_token, train.outputs['artifact'], device=serving_device, serving_container=serving_container).after(evaluate)\n",
    "    inference   = _component('viewer', 'model-inference')(auth_token, serving.outputs['servingurl'],\n",
    "                                 'digits', viewtype='inference').after(serving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(d3pipeline, 'dkube_steel_pl_full.tar.gz')\n",
    "# Upload this generated tarball into the Pipelines UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Click the dkube-training stage to see the enhanced Dkube Datascience dashboard, metrics and graphs. Click the dkube-viewer stage for the simple UI to test the model predecitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/0691edf6-901e-11ea-8248-02bcd6b2f50e\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(regression_experiment.id, 'steel_defect_pipeline', 'dkube_steel_pl_full.tar.gz', params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
