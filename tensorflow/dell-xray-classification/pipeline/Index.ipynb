{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set appropriate sys path and import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/dkube/.local/lib/python3.6/site-packages/\")\n",
    "\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List existing pipeline experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dkube ChexNet experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dell_experiment = client.create_experiment(name='dell-chexnet-pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e ChexNet Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'./create_resource.py')\n",
    "from create_resource import create_resource_job\n",
    "# from download_NIH_dataset import download\n",
    "from launch_download_job import download_job\n",
    "\n",
    "#input for create_resource\n",
    "WS_SOURCE_LINK = \"https://github.com/oneconvergence/dkube-examples/tree/dell-model-1.4.1-pipeline/tensorflow/dell-xray-classification\"\n",
    "DATASET_URL = \"https://github.com/oneconvergence/dkube-examples/tree/dell-model-1.4.1-pipeline/tensorflow/dell-xray-classification/dataset\"\n",
    "\n",
    "dkube_preprocess_op = components.load_component_from_file(\n",
    "    \"components/preprocess/component.yaml\")\n",
    "dkube_training_op = components.load_component_from_file(\n",
    "    \"components/training/component.yaml\")\n",
    "dkube_serving_op = components.load_component_from_file(\n",
    "    \"components/serving/component.yaml\")\n",
    "dkube_viewer_op = components.load_component_from_file(\n",
    "    \"components/viewer/component.yaml\")\n",
    "\n",
    "# constants\n",
    "WORKSPACE = \"chexnet-ws\"\n",
    "# input dataset for preprocessing\n",
    "PREPROCESS_DATASET = \"chexnet\"\n",
    "TARGET_DATASET = \"chexnet-preprocessed\"\n",
    "STEPS = 20000  # max no of steps\n",
    "EPOCHS = 1\n",
    "BATCHSIZE = 32\n",
    "SERVING_EXAMPLE = \"chestnet\"\n",
    "\n",
    "@dsl.pipeline(name='Dkube-ChexNet-pl',\n",
    "              description=('Dell ChexNet pipeline'\n",
    "                           'with dkube components'))\n",
    "def d3pipeline(\n",
    "    access_url,\n",
    "    auth_token,\n",
    "    user,\n",
    "    preprocess_container=json.dumps(\n",
    "        {'image': 'docker.io/ocdr/dkube-datascience-tf-cpu:v1.14'}),\n",
    "    preprocess_script=\"python preprocess.py\",\n",
    "    preprocess_program=WORKSPACE,\n",
    "    preprocess_target_name=TARGET_DATASET,  # dataset\n",
    "    # RAW dataset containing zip files of Chest X-Rays from NIH\n",
    "    preprocess_datasets=json.dumps([PREPROCESS_DATASET]),\n",
    "    training_container=json.dumps(\n",
    "        {'image': 'docker.io/ocdr/dkube-datascience-tf-gpu:v1.14'}),\n",
    "    training_program=WORKSPACE,\n",
    "    training_script=\"python model.py --ngpus=1\",\n",
    "    training_gpus=1,\n",
    "    training_envs=json.dumps([{\"steps\": STEPS,\n",
    "                               \"epochs\": EPOCHS,\n",
    "                               \"batchsize\": BATCHSIZE}])):\n",
    "\n",
    "    # create resource stage\n",
    "    create_resource_op = components.func_to_container_op(create_resource_job, base_image='docker.io/ocdr/dkube-datascience-tf-cpu:v1.14')\n",
    "    create_res = create_resource_op(user=user,\n",
    "                                         url=access_url,\n",
    "                                         token=auth_token,\n",
    "                                         ws_name=WORKSPACE,\n",
    "                                         ws_link=WS_SOURCE_LINK,\n",
    "                                         ds_name=PREPROCESS_DATASET,\n",
    "                                         ds_link=DATASET_URL)\n",
    "\n",
    "    # download dataset stage\n",
    "    download_dataset_op = components.func_to_container_op(download_job, base_image='docker.io/ocdr/dkube-datascience-tf-cpu:v1.14') \n",
    "    download_dataset = download_dataset_op(url=access_url,\n",
    "                                           user=user,\n",
    "                                           token=auth_token,\n",
    "                                           ws_name=WORKSPACE,\n",
    "                                           ds_name=PREPROCESS_DATASET).after(create_res)\n",
    "    # preprocessing stage\n",
    "    preprocess = dkube_preprocess_op(auth_token, preprocess_target_name,\n",
    "                                     preprocess_container,\n",
    "                                     program=preprocess_program,\n",
    "                                     datasets=preprocess_datasets,\n",
    "                                     run_script=preprocess_script).after(download_dataset)\n",
    "\n",
    "    # training stage\n",
    "    preprocess_dataset_name = json.dumps([str(preprocess_target_name)])\n",
    "    train = dkube_training_op(auth_token, training_container,\n",
    "                              program=training_program,\n",
    "                              run_script=training_script,\n",
    "                              datasets=preprocess_dataset_name,\n",
    "                              ngpus=training_gpus,\n",
    "                              envs=training_envs).after(preprocess)\n",
    "    # serving stage\n",
    "    serving = dkube_serving_op(\n",
    "        auth_token, train.outputs['artifact']).after(train)\n",
    "    # inference stage\n",
    "    inference = dkube_viewer_op(\n",
    "        auth_token, serving.outputs['servingurl'],\n",
    "        SERVING_EXAMPLE, viewtype='inference').after(serving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(d3pipeline, 'dell-chexnet-pl.tar.gz')\n",
    "# Upload this generated tarball into the Pipelines UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run pipeline\n",
    "Click the dkube-training stage to see the enhanced Dkube Datascience dashboard, metrics and graphs. Click the dkube-webapp stage for the simple UI to test the model predecitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(dell_experiment.id, 'dell-chexnet-pl', 'dell-chexnet-pl.tar.gz', params={'auth_token': os.getenv(\"ACCESS_TOKEN\")})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
