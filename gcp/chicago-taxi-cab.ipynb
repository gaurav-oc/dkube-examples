{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set appropriate sys path and import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client\n",
    "from kfp import components\n",
    "import json\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List existing pipeline experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dkube Dental experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dental_experiment = client.create_experiment(name='Dkube - Dental pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e Dental Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import kfp\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from kfp import gcp\n",
    "from kfp import onprem\n",
    "\n",
    "platform = 'gcp'\n",
    "\n",
    "dataflow_tf_data_validation_op  = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1f65a564d4d44fa5a0dc6c59929ca2211ebb3d1c/components/dataflow/tfdv/component.yaml')\n",
    "dataflow_tf_transform_op        = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1f65a564d4d44fa5a0dc6c59929ca2211ebb3d1c/components/dataflow/tft/component.yaml')\n",
    "tf_train_op                     = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1f65a564d4d44fa5a0dc6c59929ca2211ebb3d1c/components/kubeflow/dnntrainer/component.yaml')\n",
    "dataflow_tf_model_analyze_op    = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1f65a564d4d44fa5a0dc6c59929ca2211ebb3d1c/components/dataflow/tfma/component.yaml')\n",
    "dataflow_tf_predict_op          = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1f65a564d4d44fa5a0dc6c59929ca2211ebb3d1c/components/dataflow/predict/component.yaml')\n",
    "\n",
    "confusion_matrix_op             = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1f65a564d4d44fa5a0dc6c59929ca2211ebb3d1c/components/local/confusion_matrix/component.yaml')\n",
    "roc_op                          = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1f65a564d4d44fa5a0dc6c59929ca2211ebb3d1c/components/local/roc/component.yaml')\n",
    "\n",
    "kubeflow_deploy_op              = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1f65a564d4d44fa5a0dc6c59929ca2211ebb3d1c/components/kubeflow/deployer/component.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    column_names='gs://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json',\n",
    "    key_columns='trip_start_timestamp',\n",
    "    train='gs://ml-pipeline-playground/tfx/taxi-cab-classification/train.csv',\n",
    "    evaluation='gs://ml-pipeline-playground/tfx/taxi-cab-classification/eval.csv',\n",
    "    mode='local',\n",
    "    preprocess_module='gs://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py',\n",
    "    learning_rate=0.1,\n",
    "    hidden_layer_size='1500',\n",
    "    steps=3000,\n",
    "    analyze_slice_column='trip_start_hour'\n",
    "):\n",
    "    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\n",
    "    target_lambda = \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\"\n",
    "    target_class_lambda = \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\"\n",
    "\n",
    "    tf_server_name = 'taxi-cab-classification-model-{{workflow.uid}}'\n",
    "\n",
    "    if platform != 'GCP':\n",
    "        vop = dsl.VolumeOp(\n",
    "            name=\"create_pvc\",\n",
    "            resource_name=\"pipeline-pvc\",\n",
    "            modes=dsl.VOLUME_MODE_RWM,\n",
    "            size=\"1Gi\"\n",
    "        )\n",
    "    \n",
    "        checkout = dsl.ContainerOp(\n",
    "            name=\"checkout\",\n",
    "            image=\"alpine/git:latest\",\n",
    "            command=[\"git\", \"clone\", \"https://github.com/kubeflow/pipelines.git\", str(output) + \"/pipelines\"],\n",
    "        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n",
    "        checkout.after(vop)\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(\n",
    "        inference_data=train,\n",
    "        validation_data=evaluation,\n",
    "        column_names=column_names,\n",
    "        key_columns=key_columns,\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        validation_output=output_template,\n",
    "    )\n",
    "    if platform != 'GCP':\n",
    "        validation.after(checkout)\n",
    "\n",
    "    preprocess = dataflow_tf_transform_op(\n",
    "        training_data_file_pattern=train,\n",
    "        evaluation_data_file_pattern=evaluation,\n",
    "        schema=validation.outputs['schema'],\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        preprocessing_module=preprocess_module,\n",
    "        transformed_data_dir=output_template\n",
    "    )\n",
    "\n",
    "    training = tf_train_op(\n",
    "        transformed_data_dir=preprocess.output,\n",
    "        schema=validation.outputs['schema'],\n",
    "        learning_rate=learning_rate,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        steps=steps,\n",
    "        target='tips',\n",
    "        preprocessing_module=preprocess_module,\n",
    "        training_output_dir=output_template\n",
    "    )\n",
    "\n",
    "    analysis = dataflow_tf_model_analyze_op(\n",
    "        model=training.output,\n",
    "        evaluation_data=evaluation,\n",
    "        schema=validation.outputs['schema'],\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        slice_columns=analyze_slice_column,\n",
    "        analysis_results_dir=output_template\n",
    "    )\n",
    "\n",
    "    prediction = dataflow_tf_predict_op(\n",
    "        data_file_pattern=evaluation,\n",
    "        schema=validation.outputs['schema'],\n",
    "        target_column='tips',\n",
    "        model=training.output,\n",
    "        run_mode=mode,\n",
    "        gcp_project=project,\n",
    "        predictions_dir=output_template\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix_op(\n",
    "        predictions=prediction.output,\n",
    "        target_lambda=target_lambda,\n",
    "        output_dir=output_template\n",
    "    )\n",
    "\n",
    "    roc = roc_op(\n",
    "        predictions_dir=prediction.output,\n",
    "        target_lambda=target_class_lambda,\n",
    "        output_dir=output_template\n",
    "    )\n",
    "\n",
    "    if platform == 'GCP':\n",
    "        deploy = kubeflow_deploy_op(\n",
    "            model_dir=str(training.output) + '/export/export',\n",
    "            server_name=tf_server_name\n",
    "        )\n",
    "    else:\n",
    "        deploy = kubeflow_deploy_op(\n",
    "            cluster_name=project,\n",
    "            model_dir=str(training.output) + '/export/export',\n",
    "            pvc_name=vop.outputs[\"name\"],\n",
    "            server_name=tf_server_name\n",
    "        )\n",
    "\n",
    "    steps = [validation, preprocess, training, analysis, prediction, cm, roc, deploy]\n",
    "    for step in steps:\n",
    "        if platform == 'GCP':\n",
    "            step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "        else:\n",
    "            step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kfp.compiler.Compiler().compile(taxi_cab_classification, __file__ + '.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(d3pipeline, 'dkube_dental_pl.tar.gz')\n",
    "# Upload this generated tarball into the Pipelines UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Click the dkube-training stage to see the enhanced Dkube Datascience dashboard, metrics and graphs. Click the dkube-viewer stage for the simple UI to test the model predecitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set user, access url and auth token according to dkube setup\n",
    "project=\"\"\n",
    "output=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(dental_experiment.id, 'dental_pipeline-1', 'dkube_dental_pl.tar.gz', params={'output': output, 'project': project})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
