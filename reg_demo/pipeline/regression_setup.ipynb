{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please wait till this cell completes and then run next cells. This just need to be run once per active kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RELEASE_VERSION=0.1.18\n",
      "Collecting https://storage.googleapis.com/ml-pipeline/release/0.1.18/kfp.tar.gz\n",
      "  Using cached https://storage.googleapis.com/ml-pipeline/release/0.1.18/kfp.tar.gz (152 kB)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (5.3)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.26.0)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes<=9.0.0,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (9.0.0)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.11.3)\n",
      "Requirement already satisfied, skipping upgrade: requests_toolbelt>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.18) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.18) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (46.0.0.post20200311)\n",
      "Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.1.18) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.18) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.18) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.18) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.1.18) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth>=1.6.1->kfp==0.1.18) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (3.11.3)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (1.51.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (2019.3)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-0.1.18-py3-none-any.whl size=242154 sha256=a80dffc180c8ddaf4e55ac6e7ef0b40bef65b2528d78ec3c79923d2e82a00e3f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-42zir6pb/wheels/89/83/34/24a09b34340dfb7efecae25ca19424c615d5f46654cde53432\n",
      "Successfully built kfp\n",
      "Installing collected packages: kfp\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 0.1.18\n",
      "    Uninstalling kfp-0.1.18:\n",
      "      Successfully uninstalled kfp-0.1.18\n",
      "Successfully installed kfp-0.1.18\n"
     ]
    }
   ],
   "source": [
    "%env RELEASE_VERSION=0.1.18\n",
    "!pip install https://storage.googleapis.com/ml-pipeline/release/${RELEASE_VERSION}/kfp.tar.gz --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List existing pipeline experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiments': [{'created_at': datetime.datetime(2020, 5, 17, 6, 58, 57, tzinfo=tzlocal()),\n",
       "                  'description': 'All runs created without specifying an '\n",
       "                                 'experiment will be grouped here.',\n",
       "                  'id': '92083fb3-2150-4e39-aced-58d94c2acc30',\n",
       "                  'name': 'Default'},\n",
       "                 {'created_at': datetime.datetime(2020, 5, 17, 10, 15, 27, tzinfo=tzlocal()),\n",
       "                  'description': None,\n",
       "                  'id': '2badb595-e54f-46f4-9be5-6e239c7b7e6c',\n",
       "                  'name': 'Dkube - Regression pl'}],\n",
       " 'next_page_token': None,\n",
       " 'total_size': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dkube REGRESSION experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/2badb595-e54f-46f4-9be5-6e239c7b7e6c\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regression_experiment = client.create_experiment(name='Dkube - Regression pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e regression Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "from kfp.components._yaml_utils import load_yaml\n",
    "from kfp.components._yaml_utils import dump_yaml\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "def _component(stage, name):\n",
    "    with open('../components/{}/component.yaml'.format(stage), 'rb') as stream:\n",
    "        cdict = load_yaml(stream)\n",
    "        cdict['name'] = name\n",
    "        cyaml = dump_yaml(cdict)\n",
    "        return components.load_component_from_text(cyaml)\n",
    "        \n",
    "#dkube_preprocess_op         = components.load_component_from_file(\"../components/preprocess/component.yaml\")\n",
    "#dkube_training_op           = components.load_component_from_file(\"../components/training/component.yaml\")\n",
    "#dkube_serving_op            = components.load_component_from_file(\"../components/serving/component.yaml\")\n",
    "#dkube_viewer_op             = components.load_component_from_file('../components/viewer/component.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='dkube-regression-pl',\n",
    "    description='sample regression pipeline with dkube components'\n",
    ")\n",
    "\n",
    "def d3pipeline(\n",
    "    access_url,\n",
    "    user,\n",
    "    auth_token,\n",
    "    #Clinical preprocess\n",
    "    clinical_preprocess_script=\"python cli-pre-processing.py\",\n",
    "    clinical_preprocess_datasets=json.dumps([\"clinical\"]),\n",
    "    clinical_preprocess_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    clinical_preprocess_outputs=json.dumps([\"clinical-preprocessed\"]),\n",
    "    clinical_preprocess_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    \n",
    "    #Image preprocess\n",
    "    image_preprocess_script=\"python img-pre-processing.py\",\n",
    "    image_preprocess_datasets=json.dumps([\"images\"]),\n",
    "    image_preprocess_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    image_preprocess_outputs=json.dumps([\"images-preprocessed\"]),\n",
    "    image_preprocess_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    \n",
    "    #Clinical split\n",
    "    clinical_split_script=\"python split.py --datatype clinical\",\n",
    "    clinical_split_datasets=json.dumps([\"clinical-preprocessed\"]),\n",
    "    clinical_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    clinical_split_outputs=json.dumps([\"clinical-train\", \"clinical-test\", \"clinical-val\"]),\n",
    "    clinical_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"]),\n",
    "    \n",
    "    #Image split\n",
    "    image_split_script=\"python split.py --datatype image\",\n",
    "    image_split_datasets=json.dumps([\"images-preprocessed\"]),\n",
    "    image_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    image_split_outputs=json.dumps([\"images-train\", \"images-test\", \"images-val\"]),\n",
    "    image_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"])\t,\n",
    "    \n",
    "    #RNA split\n",
    "    rna_split_script=\"python split.py --datatype rna\",\n",
    "    rna_split_datasets=json.dumps([\"rna\"]),\n",
    "    rna_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    rna_split_outputs=json.dumps([\"rna-train\", \"rna-test\", \"rna-val\"]),\n",
    "    rna_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"]),\n",
    "    \n",
    "    #Training\n",
    "    #In notebook DKUBE_USER_ACCESS_TOKEN is automatically picked up from env variable\n",
    "    #By default tf v1.14 image is used here, v1.13 or v1.14 can be used. \n",
    "    #Or any other custom image name can be supplied.\n",
    "    #For custom private images, please input username/password\n",
    "    training_container=json.dumps({'image':'docker.io/ocdr/d3-datascience-tf-cpu:v1.14', 'username':'', 'password': ''}),\n",
    "    #Name of the workspace in dkube. Update accordingly if different name is used while creating a workspace in dkube.\n",
    "    training_program=\"regression\",\n",
    "    #Script to run inside the training container    \n",
    "    training_script=\"python train_nn.py --epochs 5\",\n",
    "    #Input datasets for training. Update accordingly if different name is used while creating dataset in dkube.    \n",
    "    training_datasets=json.dumps([\"clinical-train\", \"clinical-val\", \"images-train\",\n",
    "                                  \"images-val\", \"rna-train\", \"rna-val\"]),\n",
    "    training_input_dataset_mounts=json.dumps([\"/opt/dkube/inputs/train/clinical\", \"/opt/dkube/inputs/val/clinical\",\n",
    "                                      \"/opt/dkube/inputs/train/images\", \"/opt/dkube/inputs/val/images\",\n",
    "                                      \"/opt/dkube/inputs/train/rna\", \"/opt/dkube/inputs/val/rna\"]),\n",
    "    training_outputs=json.dumps([\"regression-model\"]),\n",
    "    training_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    #Request gpus as needed. Val 0 means no gpu, then training_container=docker.io/ocdr/dkube-datascience-tf-cpu:v1.12    \n",
    "    training_gpus=0,\n",
    "    #Any envs to be passed to the training program    \n",
    "    training_envs=json.dumps([{\"steps\": 100}]),\n",
    "    \n",
    "    #Evaluation\n",
    "    evaluation_script=\"python evaluate.py\",\n",
    "    evaluation_datasets=json.dumps([\"clinical-test\", \"images-test\", \"rna-test\"]),\n",
    "    evaluation_input_dataset_mounts=json.dumps([\"/opt/dkube/inputs/test/clinical\", \"/opt/dkube/inputs/test/images\",\n",
    "                                      \"/opt/dkube/inputs/test/rna\"]),\n",
    "    evaluation_models=json.dumps([\"regression-model\"]),\n",
    "    evaluation_input_model_mounts=json.dumps([\"/opt/dkube/inputs/model\"]),\n",
    "    \n",
    "    #Serving\n",
    "    #Device to be used for serving - dkube mnist example trained on gpu needs gpu for serving else set this param to 'cpu'\n",
    "    serving_device='cpu',\n",
    "    serving_container=json.dumps({'image':'docker.io/ocdr/dkube-regression-preprocess:2.0.5.2d86bf', 'username':'', 'password': ''})):\n",
    "    \n",
    "    create_resource = _component('setup', 'reg-setup')(access_url,\n",
    "                                               auth_token,\n",
    "                                               user)\n",
    "    \n",
    "    clinical_preprocess = _component('preprocess', 'clinical-preprocess')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=clinical_preprocess_script,\n",
    "                                      datasets=clinical_preprocess_datasets, outputs=clinical_preprocess_outputs,\n",
    "                                      input_dataset_mounts=clinical_preprocess_input_mounts, output_mounts=clinical_preprocess_output_mounts).after(create_resource)\n",
    "    image_preprocess  = _component('preprocess', 'images-preprocess')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=image_preprocess_script,\n",
    "                                      datasets=image_preprocess_datasets, outputs=image_preprocess_outputs,\n",
    "                                      input_dataset_mounts=image_preprocess_input_mounts, output_mounts=image_preprocess_output_mounts).after(create_resource)\n",
    "                                      \n",
    "    clinical_split  = _component('preprocess', 'clinical-split')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=clinical_split_script,\n",
    "                                      datasets=clinical_split_datasets, outputs=clinical_split_outputs,\n",
    "                                      input_dataset_mounts=clinical_split_input_mounts,\n",
    "                                      output_mounts=clinical_split_output_mounts).after(clinical_preprocess)\n",
    "                                      \n",
    "    image_split  = _component('preprocess', 'images-split')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=image_split_script,\n",
    "                                      datasets=image_split_datasets, outputs=image_split_outputs,\n",
    "                                      input_dataset_mounts=image_split_input_mounts,\n",
    "                                      output_mounts=image_split_output_mounts).after(image_preprocess)\n",
    "                                      \n",
    "    rna_split  = _component('preprocess', 'rna-split')(auth_token, training_container,\n",
    "                                      program=training_program, run_script=rna_split_script,\n",
    "                                      datasets=rna_split_datasets, outputs=rna_split_outputs,\n",
    "                                      input_dataset_mounts=rna_split_input_mounts, output_mounts=rna_split_output_mounts).after(create_resource)\n",
    "                                      \n",
    "    train       = _component('training', 'regression-model-training')(auth_token, training_container,\n",
    "                                    program=training_program, run_script=training_script,\n",
    "                                    datasets=training_datasets, outputs=training_outputs,\n",
    "                                    input_dataset_mounts=training_input_dataset_mounts,\n",
    "                                    output_mounts=training_output_mounts,\n",
    "                                    ngpus=training_gpus,\n",
    "                                    envs=training_envs).after(clinical_split).after(image_split).after(rna_split)\n",
    "    evaluate    = _component('training', 'model-eval')(auth_token, training_container,\n",
    "                                    program=training_program, run_script=evaluation_script,\n",
    "                                    datasets=evaluation_datasets,\n",
    "                                    input_dataset_mounts=evaluation_input_dataset_mounts,\n",
    "                                    models=evaluation_models,\n",
    "                                    input_model_mounts=evaluation_input_model_mounts,\n",
    "                                    ngpus=training_gpus,\n",
    "                                    envs=training_envs).after(train)\n",
    "    serving     = _component('serving', 'model-serving')(auth_token, train.outputs['artifact'], device=serving_device, serving_container=serving_container).after(evaluate)\n",
    "    #inference   = dkube_viewer_op(auth_token, serving.outputs['servingurl'],\n",
    "    #                              'digits', viewtype='inference').after(serving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(d3pipeline, 'dkube_regression_pl_full.tar.gz')\n",
    "# Upload this generated tarball into the Pipelines UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Click the dkube-training stage to see the enhanced Dkube Datascience dashboard, metrics and graphs. Click the dkube-viewer stage for the simple UI to test the model predecitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/0691edf6-901e-11ea-8248-02bcd6b2f50e\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(regression_experiment.id, 'regression_classifier_pipeline', 'dkube_regression_pl_2.tar.gz', params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
